{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163701f-21fa-43b1-940b-554436ceafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.geometric.nn import SAGEConv\n",
    "from torch.geometric.datasets import DGraphFin\n",
    "from torch.geometric.loader import NeighborLoader\n",
    "from torch.geometric.transforms import RandomNodeSplit\n",
    "from torch.geometric.data import Data\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e5591-cfa4-427c-b9ea-0360163b00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is.available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c716fb2-8a95-4ab9-a599-3655c84c12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/Users/kostas/Documents/GitHub/SMCS_FraudDetection/dataset\" \n",
    "dataset = DGraphFin(root=DATASET_PATH)[0].to(DEVICE)\n",
    "#remove unused attributes\n",
    "dataset.pop(\"edge_type\")\n",
    "dataset.pop(\"edge_time\")\n",
    "#normalize features (z-score)\n",
    "dataset.x = (dataset.x - dataset.x.mean(dim=0))/dataset.x.std(dim=0)\n",
    "NUM_CLASSES = dataset.y.max().item()+1\n",
    "NUM_FEATURES = dataset.num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559efad1-9b25-4536-b845-253432cdcdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "train_loader = NeighborLoader(dataset, num_neighbors=[10,5], batch_size=BATCH_SIZE, input_nodes= dataset.train_mask, shuffle=True)\n",
    "test_loader = NeighborLoader(dataset, num_neighbors=[10,5], batch_size=BATCH_SIZE, input_nodes= dataset.test_mask, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "141b4749-7241-4ef0-b216-43a9aeaf097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2): \n",
    "\n",
    "    super(GraphSAGE, self).__init__() \n",
    "    self.num_layers = num_layers \n",
    "    self.convs = torch.nn.ModuleList() \n",
    "    # Input layer \n",
    "    self.convs.append(SAGEConv(in_channels, hidden_channels)) \n",
    "    # Hidden layers \n",
    "    for _ in range(num_layers - 2): \n",
    "        self.convs.append(SAGEConv(hidden_channels, hidden_channels)) \n",
    "    # Output layer \n",
    "    self.convs.append(SAGEConv(hidden_channels, out_channels)) \n",
    "\n",
    "def forward(self, x, edge_index): \n",
    "    for conv in self.convs[:-1]: \n",
    "        x = conv(x, edge_index) \n",
    "        x = F.relu(x) \n",
    "        x = F.dropout(x, p=0.5, training=self.training) \n",
    "    x = self.convs[-1](x, edge_index) \n",
    "    return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597b0a4-4413-4cf6-abb7-a956b2d91f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphSAGE(in_channels=NUM_FEATURES, hidden_channels=128, out_channels=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, weight_decay=5e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095cb524-db9a-45fc-911b-0a62e1bde869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0   \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):   \n",
    "        batch = batch.to(DEVICE)   \n",
    "        optimizer.zero_grad()   \n",
    "        out = model(batch.x, batch.edge_index)   \n",
    "        loss = loss_fn(out[batch.train_mask], batch.y[batch.train_mask])     \n",
    "        loss.backward()     \n",
    "        optimizer.step()   \n",
    "        total_loss += loss.item()  \n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a63069e-07e6-4cca-909c-51c76edc90f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds, all_labels = [],[]\n",
    "\n",
    "    for batch in test_loader:    \n",
    "        batch = batch.to(DEVICE)    \n",
    "        out = model(batch.x, batch.edge_index)    \n",
    "        pred = out.argmax(dim=1)    \n",
    "        correct += (pred[batch.test_mask] == batch.y[batch.test_mask]).sum().item()    \n",
    "        total += batch.test_mask.sum().item()    \n",
    "        all_preds.append(pred[batch.test_mask].cpu())    \n",
    "        all_labels.append(batch.y[batch.test_mask].cpu())    \n",
    "    \n",
    "    # Compute accuracy    \n",
    "    accuracy = correct / total    \n",
    "        \n",
    "    # Compute F1-score    \n",
    "    all_preds = torch.cat(all_preds)    \n",
    "    all_labels = torch.cat(all_labels)    \n",
    "    f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average=\"micro\")    \n",
    "        \n",
    "    return accuracy, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ff16f2e-d9f6-4303-9626-eb2bfa99cb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2530 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> 7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     acc, f1 \u001b[38;5;241m=\u001b[39m test()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      3\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pyg\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pyg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pyg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pyg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:620\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pyg\\lib\\site-packages\\torch\\utils\\data\\sampler.py:283\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m    282\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[0;32m    284\u001b[0m     batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[0;32m    285\u001b[0m     idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pyg\\lib\\site-packages\\torch\\utils\\data\\sampler.py:165\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n):\n\u001b[1;32m--> 165\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mrandperm(n, generator\u001b[38;5;241m=\u001b[39mgenerator)[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m%\u001b[39m n]\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train()\n",
    "    acc, f1 = test()\n",
    "    print(f\"Epoch{epoch+1}/{EPOCHS}, Loss: {loss:.4f}, Accuracy: {acc:.4f},F1-score:{f1:4f}\")\n",
    "print(\"training completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe14a2d6-31e7-4a37-9394-0948631d20ea",
   "metadata": {},
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f17624-09b8-4efd-ac56-66d6eac11f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from data_processing import load_dgraphfin, split_into_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d81a4-1228-414b-89bd-1cb52db14fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ee535-845f-4ba2-8c42-61689a5719f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, num_classes = load_dgraphfin(path_to_folder=\"/Users/kostas/Documents/GitHub/SMCS_FraudDetection/\")\n",
    "\n",
    "# Step 2: Create Data Loaders\n",
    "train_loader = split_into_batches(graph=dataset, num_batches=512, num_neighbours=10, num_hops=2, shuffle=True)\n",
    "test_loader = split_into_batches(graph=dataset, num_batches=512, num_neighbours=10, num_hops=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a662b6-07d5-4ee2-b090-8083ab8cef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the GraphSAGE Model\n",
    "class GraphSAGEModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.conv1 = GraphSAGE(in_channels, hidden_channels, num_layers=2, out_channels=hidden_channels)\n",
    "        self.conv2 = GraphSAGE(hidden_channels, hidden_channels, num_layers=2, out_channels=out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e925a24-5428-4cf5-8a09-19cf3aa7c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = GraphSAGEModel(dataset.num_features, hidden_channels=64, out_channels=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e6421-aafb-406a-aed5-e06d644b701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    for batch in tqdm.tqdm(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "\n",
    "        loss = F.cross_entropy(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * batch.train_mask.sum().item()\n",
    "        total_examples += batch.train_mask.sum().item()\n",
    "\n",
    "    return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8d6e0-7f08-423e-a5d2-6338b75c278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define Evaluation Function\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        preds.append(out.argmax(dim=1)[batch.test_mask].cpu())\n",
    "        labels.append(batch.y[batch.test_mask].cpu())\n",
    "\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    f1 = f1_score(labels.numpy(), preds.numpy(), average=\"micro\")\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "835ce49d-6bd4-4ef8-afa0-16f9773fd822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e860a8a-4a8d-450f-8ab8-86ace7d80239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Train and Evaluate the Model\n",
    "for epoch in range(1, 11):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    f1_score_test = test(test_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss:.4f}, Test F1: {f1_score_test:.4f}, Time: {time.time() - start:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87401ca9-7878-4ed0-ba0f-12b502d74073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import torch\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "\n",
    "def check_sampling_dependencies() -> bool:\n",
    "    \"\"\"\n",
    "    Checks if required dependencies for neighbor sampling are installed.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if either pyg-lib or torch-sparse is available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pyg_lib\n",
    "        return True\n",
    "    except ImportError:\n",
    "        try:\n",
    "            import torch_sparse\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "\n",
    "def create_neighbor_sampler(\n",
    "    edge_index: torch.Tensor,\n",
    "    sizes: list[int],\n",
    "    num_nodes: int,\n",
    "    batch_size: int,\n",
    "    shuffle: bool = True\n",
    ") -> Union[NeighborSampler, None]:\n",
    "    \"\"\"\n",
    "    Creates a NeighborSampler instance with proper dependency checking.\n",
    "    \n",
    "    Args:\n",
    "        edge_index (torch.Tensor): Graph edge indices\n",
    "        sizes (list[int]): Number of neighbors to sample per layer\n",
    "        num_nodes (int): Total number of nodes in the graph\n",
    "        batch_size (int): Size of batches\n",
    "        shuffle (bool, optional): Whether to shuffle the nodes. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        Union[NeighborSampler, None]: NeighborSampler instance if dependencies are met, None otherwise\n",
    "    \n",
    "    Raises:\n",
    "        ImportError: If neither pyg-lib nor torch-sparse is installed\n",
    "    \"\"\"\n",
    "    if not check_sampling_dependencies():\n",
    "        raise ImportError(\n",
    "            \"NeighborSampler requires either 'pyg-lib' or 'torch-sparse'. \"\n",
    "            \"Please install at least one of them:\\n\"\n",
    "            \"pip install pyg-lib torch-sparse\\n\"\n",
    "            \"or\\n\"\n",
    "            \"conda install pyg -c pyg\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        return NeighborSampler(\n",
    "            edge_index,\n",
    "            node_idx=torch.arange(num_nodes),\n",
    "            sizes=sizes,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating NeighborSampler: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Usage example:\n",
    "try:\n",
    "    train_loader = create_neighbor_sampler(\n",
    "        edge_index=data.edge_index,\n",
    "        sizes=[25, 10],  # Sample 25 neighbors for first hop, 10 for second hop\n",
    "        num_nodes=data.num_nodes,\n",
    "        batch_size=128\n",
    "    )\n",
    "    if train_loader is None:\n",
    "        raise RuntimeError(\"Failed to create NeighborSampler\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"Dependency Error: {str(e)}\")\n",
    "    # Handle the error appropriately (e.g., fall back to a different sampling method)\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519a243-1528-4000-acf7-ff22a6e1963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DGraphFin\n",
    "from utils.utils import prepare_folder\n",
    "from utils.evaluator import Evaluator\n",
    "from models import MLP, MLPLinear, GCN, SAGE, GAT, GATv2\n",
    "from logger import Logger\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.utils import to_undirected\n",
    "import pandas as pd\n",
    "\n",
    "sage_parameters = {'lr':0.01\n",
    "              , 'num_layers':2\n",
    "              , 'hidden_channels':128\n",
    "              , 'dropout':0\n",
    "              , 'batchnorm': False\n",
    "              , 'l2':5e-7\n",
    "             }\n",
    "def train(model, data, train_idx, optimizer, no_conv=False):\n",
    "    # data.y is labels of shape (N, ) \n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    if no_conv:\n",
    "        out = model(data.x[train_idx])\n",
    "    else:\n",
    "        out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f616170-ab59-4dbb-999d-e44438afc628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
