{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import *\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "train_data, OUT_DIM = load_create_ellipticpp(timestep=(1,32))\n",
    "train_data = train_data.to(DEVICE)\n",
    "test_data, OUT_DIM = load_create_ellipticpp(timestep=(33,37))\n",
    "test_data = test_data.to(DEVICE)\n",
    "valid_data, OUT_DIM = load_create_ellipticpp(timestep=(38,42))\n",
    "valid_data = test_data.to(DEVICE)\n",
    "# loader = split_into_batches(data, num_batches=batch_size, num_hops=2, num_neighbours=100)\n",
    "# num_features = next(iter(loader)).x.shape[-1]\n",
    "# data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe remvoe mask and cosnider temporal split for elliptic++\n",
    "# dataloader (from pt) - implement sampler here -> neighourloader\n",
    "# summary stats on avg degree of a node when considering ([-1,-1] neighbourhoods)\n",
    "# graphsage/gat with full neighbourhood, 75%/50%/25% reduction in neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Sampler\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from typing import Iterator, Iterable, List\n",
    "\n",
    "# class customGraphSampler(Sampler[List[int]]):\n",
    "#     def __init__(self, data: Iterable, batch_size: int=32, positive_label: int=1, negative_label: int=0) -> None:\n",
    "#         self.data = data\n",
    "#         self.batch_size = batch_size\n",
    "#         self.positive_label = positive_label\n",
    "#         self.negative_label = negative_label\n",
    "\n",
    "#     # number of batches\n",
    "#     # @TODO: recompute based on labels\n",
    "#     def __len__(self) -> int:\n",
    "#         return (len(self.data.x) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "#     def _is_half_batch_size(self, counter_sample) -> int:\n",
    "#         return counter_sample >= self.batch_size // 2\n",
    "\n",
    "#     def __iter__(self) -> Iterator[int]:\n",
    "#         batch = []\n",
    "#         # torch where y == 0, y == 1 -> select first batch_size idx `y == 0` and batch_size - selection_size `y == 1`\n",
    "        \n",
    "#         # ****@TODO: iterate over the entire dataset and add to single list? -- and discard non relevant batches\n",
    "#         ### get all neighbours\n",
    "#         # @TODO: iterate over the entire dataset but keep some other batches as well\n",
    "#         # @TODO: accumulate positive and negative in separate lists and then combine?\n",
    "#         # @TODO: sample from two lists and add indices\n",
    "\n",
    "#         # @TODO: shuffle indices?\n",
    "#         self.sizes = 2\n",
    "\n",
    "#         # [1,0,1,0,2_]\n",
    "#         # pos = 2\n",
    "#         # neg = 2\n",
    "#         # idx = 4 -> idx+1 = 5\n",
    "#         # len = 5\n",
    "\n",
    "#         counter_positive_label = 0\n",
    "#         counter_negative_label = 0\n",
    "#         for idx, label in enumerate(self.data.y):\n",
    "#             if label == self.positive_label and not self._is_half_batch_size(counter_positive_label) and idx in self.data.edge_index[0]:\n",
    "#                 batch += [idx]\n",
    "#                 counter_positive_label += 1\n",
    "#             elif label == self.negative_label and not self._is_half_batch_size(counter_negative_label) and idx in self.data.edge_index[0]:\n",
    "#                 batch += [idx]\n",
    "#                 counter_negative_label += 1\n",
    "\n",
    "#             if (len(batch) == self.batch_size):\n",
    "#                 # print(batch)\n",
    "#                 # yield self.data.subgraph(torch.as_tensor(batch))\n",
    "#                 yield batch\n",
    "#                 batch = []\n",
    "#                 counter_positive_label = 0\n",
    "#                 counter_negative_label = 0\n",
    "\n",
    "# # get node idx (Y) -> make batch (Y) -> get ALL neibghours of idx (in n hops)\n",
    "# # separate graph into subgraphs (where each subgraph = 64 nodes + neighbours) -> make batches ...\n",
    "\n",
    "# # s = customGraphSampler(d, batch_size=6, positive_label=0, negative_label=1) # generate indices to form a batch\n",
    "# # da = DataLoader(dataset=d.x, sampler=s, batch_size=5) # go over the entire dataset (here, list of node values) and create a batch from the sampled indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import Iterator, Iterable, List\n",
    "\n",
    "class customGraphSampler(Sampler[List[int]]):\n",
    "    def __init__(self, data: Iterable, batch_size: int=32, positive_label: int=1, negative_label: int=0) -> None:\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.positive_label = positive_label\n",
    "        self.negative_label = negative_label\n",
    "\n",
    "        self.y_positive_idx = torch.where(self.data.y == positive_label, 1, 0).nonzero().squeeze()\n",
    "        self.y_positive_idx = self.y_positive_idx[torch.randperm(self.y_positive_idx.size()[0])]\n",
    "\n",
    "        self.y_negative_idx = torch.where(self.data.y == negative_label, 1, 0).nonzero().squeeze()\n",
    "        self.y_negative_idx = self.y_negative_idx[torch.randperm(self.y_negative_idx.size()[0])]\n",
    "\n",
    "    # number of batches\n",
    "    # @TODO: recompute based on labels\n",
    "    def __len__(self) -> int:\n",
    "        return (len(self.data.x) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def _is_half_batch_size(self, counter_sample) -> int:\n",
    "        return counter_sample >= self.batch_size // 2\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        batch = []\n",
    "        # torch where y == 0, y == 1 -> select first batch_size idx `y == 0` and batch_size - selection_size `y == 1`\n",
    "        \n",
    "        # ****@TODO: iterate over the entire dataset and add to single list? -- and discard non relevant batches\n",
    "        ### get all neighbours\n",
    "        # @TODO: iterate over the entire dataset but keep some other batches as well\n",
    "        # @TODO: accumulate positive and negative in separate lists and then combine?\n",
    "        # @TODO: sample from two lists and add indices\n",
    "\n",
    "        # @TODO: shuffle indices?\n",
    "\n",
    "        # print(self.y_positive_idx.shape)\n",
    "        # print(self.y_negative_idx.shape)\n",
    "\n",
    "        aux_y_positive_idx = self.y_positive_idx\n",
    "        aux_y_negative_idx = self.y_negative_idx\n",
    "\n",
    "        half_batch_size = self.batch_size // 2\n",
    "\n",
    "        while aux_y_positive_idx.shape[0] >= half_batch_size and aux_y_negative_idx.shape[0] >= half_batch_size:\n",
    "            positive_labels = aux_y_positive_idx[:half_batch_size]\n",
    "            aux_y_positive_idx = aux_y_positive_idx[half_batch_size:]\n",
    "\n",
    "            negative_labels = aux_y_negative_idx[:half_batch_size]\n",
    "            aux_y_negative_idx = aux_y_negative_idx[half_batch_size:]\n",
    "\n",
    "            batch = torch.concat([positive_labels, negative_labels], dim=-1)\n",
    "\n",
    "            yield batch\n",
    "\n",
    "# add hops to get neighbours of neighbours\n",
    "def select_nodes_from_central(data: Data, nodes: torch.Tensor):\n",
    "   \n",
    "    edge_mask = [data.edge_index[:,torch.where(data.edge_index == node, 1, 0).nonzero().squeeze(-1)[:,1]] for node in nodes]\n",
    "    edge_index = torch.concat(edge_mask, dim=1)\n",
    "\n",
    "    node_new_idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(edge_index.unique().tolist())}\n",
    "    for old_idx, new_idx in node_new_idx_map.items():\n",
    "        edge_index[edge_index == old_idx] = new_idx\n",
    "\n",
    "    all_old_nodes_idx = list(node_new_idx_map.keys())\n",
    "\n",
    "    return edge_index, all_old_nodes_idx\n",
    "\n",
    "# @TODO: change sample - incorrect choosing; mask must be extended to have only 64 ones and the others (for structure nodes) be 0\n",
    "\n",
    "def split(data: Data, sampler: Sampler, positive_label: int=0, negative_label: int=1, max_batches: int=100):\n",
    "    batch_graph = []\n",
    "\n",
    "    for idx, sample in enumerate(sampler):\n",
    "        edge_index, old_idx = select_nodes_from_central(data, sample)\n",
    "        train_mask = data.y[old_idx]\n",
    "        train_mask[train_mask == positive_label] = True \n",
    "        train_mask[(train_mask != positive_label) & (train_mask != negative_label)] = False\n",
    "        batch_graph += [\n",
    "            Data(x=data.x[old_idx,:],\n",
    "                 y=data.y[old_idx],\n",
    "                 edge_index=edge_index,\n",
    "                 train_mask = train_mask.to(bool))\n",
    "        ]\n",
    "        if max_batches != -1 and idx + 1 >= max_batches:\n",
    "            break\n",
    "        # print(\"------\")\n",
    "        # print(sample.shape[0])\n",
    "        # print(sum([s.item() in old_idx for s in sample])/sample.shape[0])\n",
    "        # print(data.y[old_idx].tolist())\n",
    "        # print(batch_graph[0].train_mask.tolist())\n",
    "\n",
    "    return batch_graph\n",
    "\n",
    "# get node idx (Y) -> make batch (Y) -> get ALL neibghours of idx (in n hops)\n",
    "# separate graph into subgraphs (where each subgraph = 64 nodes + neighbours) -> make batches ...\n",
    "\n",
    "# s = customGraphSampler(d, batch_size=6, positive_label=0, negative_label=1) # generate indices to form a batch\n",
    "# da = DataLoader(dataset=d.x, sampler=s, batch_size=5) # go over the entire dataset (here, list of node values) and create a batch from the sampled indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# use full edge_index tensor\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# @TODO: test bidirectional edges\u001b[39;00m\n\u001b[32m     35\u001b[39m ex = torch.concat([s.edge_index, torch.as_tensor([[\u001b[32m999\u001b[39m, \u001b[32m139\u001b[39m],[\u001b[32m999\u001b[39m, \u001b[32m999\u001b[39m]], device=DEVICE)], dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m a = \u001b[43mget_neighbours_hop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m139\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m280\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hops\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m a\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mget_neighbours_hop\u001b[39m\u001b[34m(edge_index, source_node, target_node, num_hops)\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m aux_connected_node.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m target_node \u001b[38;5;129;01min\u001b[39;00m aux_connected_node:\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     connected_nodes_list = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconnected_nodes_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_neighbours_hop\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hops\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connected_nodes_list\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "def get_connected_nodes(edge_index: torch.Tensor, source_node, target_node):\n",
    "    current_direction_mask = torch.where((edge_index.T == torch.as_tensor([source_node, target_node], device=DEVICE)) | (edge_index.T == torch.as_tensor([target_node, source_node], device=DEVICE)), 1, 0).sum(1) != 2\n",
    "    edge_index_aux = edge_index[:,current_direction_mask]\n",
    "    edge_mask = torch.where(edge_index_aux == source_node, 1, 0).nonzero().squeeze(-1)[:,1]\n",
    "    connected_nodes = edge_index_aux[:,edge_mask]\n",
    "    # print(torch.where((edge_index == source_node) & (ed), 1, 0))\n",
    "\n",
    "    return connected_nodes.to(DEVICE)\n",
    "\n",
    "def get_neighbours_hop(edge_index: torch.Tensor, source_node: int, target_node: int, num_hops: int=2) -> list:\n",
    "    if num_hops <= 1:\n",
    "        return torch.Tensor()\n",
    "    \n",
    "    connected_nodes_list = torch.Tensor()\n",
    "    connected_nodes = get_connected_nodes(edge_index, source_node, target_node)\n",
    "    \n",
    "    unique_nodes = connected_nodes.unique().tolist()\n",
    "    unique_nodes.remove(source_node)\n",
    "\n",
    "    # print(connected_nodes)\n",
    "    print(\"----\")\n",
    "\n",
    "    connected_nodes_list = connected_nodes\n",
    "    for node in unique_nodes:\n",
    "        aux_connected_node = get_connected_nodes(edge_index, target_node, node)\n",
    "        # print(aux_connected_node)\n",
    "        if aux_connected_node.shape[1] == 1 and target_node in aux_connected_node:\n",
    "            continue\n",
    "        connected_nodes_list = torch.concat([connected_nodes_list, get_neighbours_hop(edge_index, target_node, node, num_hops-1)])\n",
    "\n",
    "    return connected_nodes_list\n",
    "\n",
    "# use full edge_index tensor\n",
    "# @TODO: test bidirectional edges\n",
    "ex = torch.concat([s.edge_index, torch.as_tensor([[999, 139],[999, 999]], device=DEVICE)], dim=1)\n",
    "a = get_neighbours_hop(ex, 139, 280, num_hops=2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = customGraphSampler(train_data, batch_size=64, positive_label=0, negative_label=1) # generate indices to form a batch\n",
    "subgraphs = split(train_data, batch_sampler, positive_label=0, negative_label=1, max_batches=50)\n",
    "train_data_batches = DataLoader(subgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = customGraphSampler(test_data, batch_size=64, positive_label=0, negative_label=1) # generate indices to form a batch\n",
    "subgraphs = split(test_data, batch_sampler, positive_label=0, negative_label=1, max_batches=50)\n",
    "test_data_batches = DataLoader(subgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = next(iter(train_data_batches))\n",
    "s.y[s.train_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove timestep attribute !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# sample = next(iter(loader))\n",
    "# (sample.y.tolist())\n",
    "\n",
    "# Counter(sample.y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.datasets import elliptic\n",
    "\n",
    "# da = elliptic.EllipticBitcoinDataset(\"./\").data.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = split_into_batches(da, num_batches=100000, num_neighbours=100, num_hops=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced dataset -> try to sample in a batch 64 central nodes - 32 class 0, 32 class 1\n",
    "# use 2 neighbourloaders -> one for central node with custom sampling, one with random for neighbours (using node idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, **layer_paras):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = layer_paras.pop(\"num_layers\",1)\n",
    "        self.hidden_dim = layer_paras.pop(\"hidden_channels\")\n",
    "        self.cached = layer_paras.pop(\"cached\", True)\n",
    "\n",
    "        self.dropout = layer_paras.pop(\"dropout\", 0.0)\n",
    "\n",
    "        self.conv_layers = []\n",
    "\n",
    "        self.conv_layers += [\n",
    "            SAGEConv(in_dim, self.hidden_dim) # input layer; cached=True => for transductive learning\n",
    "        ]\n",
    "        for _ in range(self.num_layers-1):\n",
    "            self.conv_layers += [\n",
    "                SAGEConv(self.hidden_dim, self.hidden_dim)\n",
    "            ]\n",
    "        self.conv_layers += [\n",
    "            SAGEConv(self.hidden_dim, out_dim) # output layer; cached=True => for transductive learning\n",
    "        ]\n",
    "\n",
    "        self.conv_layers = torch.nn.ParameterList(self.conv_layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.conv_layers[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=self.dropout)\n",
    "        x = self.conv_layers[-1](x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAGE(\n",
       "  (conv_layers): ParameterList(\n",
       "      (0): Object of type: SAGEConv\n",
       "      (1): Object of type: SAGEConv\n",
       "    (0): SAGEConv(55, 55, aggr=mean)\n",
       "    (1): SAGEConv(55, 2, aggr=mean)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras = {\n",
    "    'lr':0.01,\n",
    "    'num_layers':1,\n",
    "    'hidden_channels':55,\n",
    "    'dropout':0.4,\n",
    "    'batchnorm': False,\n",
    "    'l2':5e-7,\n",
    "    'cached': True,\n",
    "}\n",
    "model = SAGE(in_dim=55, out_dim=2, **paras).to(DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1; Batch: 575; Loss=368.8005; Loss-Validation=0.0000: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(batches, model, optimizer, epoch_num=1):\n",
    "    loss_all = []\n",
    "    loss_valid_all = []\n",
    "\n",
    "    model.train()\n",
    "    iterator = tqdm(range(epoch_num), desc=\"\")\n",
    "    for epoch in iterator:\n",
    "        running_loss = 0\n",
    "        valid_loss = 0\n",
    "        for idx, batch in enumerate(batches):\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "\n",
    "            loss = F.cross_entropy(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iterator.set_description(f\"Epoch: {epoch+1}/{epoch_num}; Batch: {idx+1}/{len(subgraphs)}; Loss: {running_loss/(idx+1):0.4f}\")\n",
    "\n",
    "        # loss_valid = F.cross_entropy(out[batch.val_mask], batch.y[batch.val_mask])\n",
    "        # running_valid_loss += loss_valid.item()    \n",
    "        # iterator.set_description(f\"Epoch: {epoch+1}/{epoch_num}; Batch: {idx+1}/{data.x.shape[0]//batch_size}; Loss: {running_loss/(idx+1):0.4f}\")\n",
    "        # @TODO: add validation round to monitor performance\n",
    "        # loss_all += [running_loss/(data.x.shape[0]//batch_size)]\n",
    "        # loss_valid_all += [running_valid_loss/(data.x.shape[0]//batch_size)]\n",
    "\n",
    "        loss_all += [running_loss/(idx+1)]\n",
    "        loss_valid_all += [valid_loss]\n",
    "\n",
    "        iterator.set_description(f\"Epoch: {epoch+1}/{epoch_num}; Batch: {idx}; Loss={loss_all[-1]:.4f}; Loss-Validation={loss_valid_all[-1]:.4f}\")\n",
    "\n",
    "    return loss_all, loss_valid_all\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=paras[\"lr\"], weight_decay=paras[\"l2\"])\n",
    "\n",
    "loss_all, loss_valid_all = train(train_data_batches, model, optimizer, epoch_num=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x73956b375dc0>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_all)\n",
    "# plt.plot(loss_valid_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7050285935401917\n",
      "Precision=inf\n",
      "Recall=1.0\n",
      "F1=nan\n",
      "Auroc=nan\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.explain.metric import groundtruth_metrics\n",
    "\n",
    "def compute_cm(orig_labels, pred_labels, positive_label, negative_label):\n",
    "    tp = ((orig_labels == pred_labels) & (orig_labels == positive_label)).sum()\n",
    "    tn = ((orig_labels == pred_labels) & (orig_labels == negative_label)).sum()\n",
    "    fn = ((orig_labels != pred_labels) & (orig_labels == positive_label)).sum()\n",
    "    fp = ((orig_labels != pred_labels) & (orig_labels == negative_label)).sum()\n",
    "\n",
    "    return torch.as_tensor([[tp, fn], [fp, tn]])\n",
    "\n",
    "def compute_accuracy(cm: torch.Tensor):\n",
    "    return (cm.diagonal().sum() / cm.sum()).item()\n",
    "\n",
    "def compute_precision(cm):\n",
    "    return (cm[0].sum() / cm[:,0].sum()).item()\n",
    "\n",
    "def compute_recall(cm):\n",
    "    return (cm[0].sum() / cm[0,:].sum()).item()\n",
    "\n",
    "def compute_f1(cm, precision=None, recall=None):\n",
    "    if not (precision and recall):\n",
    "        precision = compute_precision(cm)\n",
    "        recall = compute_precision(cm)\n",
    "\n",
    "    return (2*precision*recall)/(precision+recall)\n",
    "\n",
    "def compute_metrics(cm):\n",
    "    metrics = [compute_accuracy, compute_precision, compute_recall, compute_f1]\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        results[metric.__name__.split(\"_\")[-1]] = metric(cm)\n",
    "\n",
    "    return results\n",
    "\n",
    "avg_scores = []\n",
    "model.eval()\n",
    "\n",
    "acc = []\n",
    "metrics = {\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": [],\n",
    "    \"auroc\": [],\n",
    "}\n",
    "positive_label = 0\n",
    "negative_label = 1\n",
    "\n",
    "for batch in test_data_batches:\n",
    "    pred = model(batch.x, batch.edge_index)\n",
    "    pred_labels = pred.argmax(dim=1)[batch.train_mask]\n",
    "    orig_labels = batch.y[batch.train_mask]\n",
    "\n",
    "    results = compute_metrics(compute_cm(orig_labels, pred_labels, positive_label=positive_label, negative_label=negative_label))\n",
    "    for metric, score in results.items():\n",
    "        metrics[metric] += [score]\n",
    "\n",
    "    # acc += [(orig_labels == pred_labels).sum()/orig_labels.shape[0]]\n",
    "\n",
    "\n",
    "    # scores = groundtruth_metrics(\n",
    "    #     pred_mask=pred[batch.train_mask].max(dim=1)[1],\n",
    "    #     target_mask=batch.y[batch.train_mask],\n",
    "    #     threshold=0.5,\n",
    "    #     metrics=[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
    "    # )\n",
    "    # avg_scores += [scores]\n",
    "\n",
    "for metric, scores in metrics.items():\n",
    "    print(f\"{metric.title()}={torch.mean(torch.as_tensor(scores))}\")\n",
    "# import numpy as np\n",
    "# print(np.mean(acc))\n",
    "\n",
    "# np.asarray(avg_scores).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 36],\n",
       "        [ 0, 77]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cm(orig_labels, pred_labels, positive_label=positive_label, negative_label=negative_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.70515263\n",
    "# array([0.70515261, 0.70515261, 1.        , 0.82589429])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain.metric import groundtruth_metrics\n",
    "\n",
    "avg_scores = []\n",
    "model.eval()\n",
    "skips = 0\n",
    "for batch in tqdm(loader, total=data.x.shape[0]//batch_size):\n",
    "    if batch.test_mask.sum() <= 0:\n",
    "        skips += 1\n",
    "        continue\n",
    "    pred = model(batch.x, batch.edge_index).argmax(dim=1)\n",
    "\n",
    "    scores = groundtruth_metrics(\n",
    "        pred_mask=pred[batch.test_mask],\n",
    "        target_mask=batch.y[batch.test_mask],\n",
    "        threshold=0.5,\n",
    "        metrics=[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
    "    )\n",
    "    avg_scores += [scores]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Skips = {skips}\")\n",
    "np.asarray(avg_scores).mean(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
